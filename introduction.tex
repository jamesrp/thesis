%!TEX root = thesis.tex


\pagenumbering{arabic}

\chapter{Introduction}
\label{chap:Introduction}

\section{The Theta Body of the Stable Set Problem}
This thesis studies the application of sums of squares to combinatorial
optimization. Chapters 2-X each cover a different aspect of this research.
In this introduction, we give a case study in which we apply sums of squares
to the stable set problem, and introduce many of the concepts which will be 
used in the following chapters. Then we give an extended abstract of each
chapter.

\subsection{Combinatorial Optimization Problems}
Combinatorial optimization is the branch of optimization problems concerning
finding the best object from a discrete set. Applications are common in 
computer science, mathematics, and operations research. For instance, the 
objective of traveling salesman problem is to find a minimum-cost tour among
a set of cities; the shortest path problem is to find a shortest-distance
path between two nodes in a graph; and the assignment problem is to assign
workers to jobs while minimizing the total wages paid. In particular, we will
consider the 
stable set problem, where we find the maximum-size independent vertex set in a
graph.

Combinatorial optimization problems often end up being NP-complete or harder.
That is, we are unlikely to ever find efficient algorithms to solve them
exactly. And in some cases where the abstract mathematical problem does have
an efficient algorithm, real-world constraints can make it intractable. For
instance, when assigning medical students to residencies, the constraint that
married couples live in the same city makes the problem NP-complete.

Therefore, much research has focused on developing methods to solve these
problems approximately. In this paper, we describe one such method based on
sums of squares.

\subsection{Sums of Squares and Optimization}
Consider for the moment unconstrained optimization problems on $\RR^n$. That 
is, we are given a function $f$ in $n$ real variables, and must find the 
global minimum value $f_{\min}$. We can rephrase this as $f_{\min} = 
\max \{r: f(x) - r \ge 0\}$. Now if we can write $f(x) - r$ as a sum of squares,
then it is certainly nonnegative. Therefore, we can define the relaxation
$f_{\textup{sos}} = \max \{r: f(x) - r \textup{ is a sum of squares}\}$.
We have that $f_{\min} \ge f_{\textup{sos}}$, so this is a lower bound on
the minimum. This idea is discussed at more length in \cite{sostools} and 
\cite{lasserre}. We now specialize to the case of combinatorial optimization.

\subsection{The Theta Body}
We would like to apply the $f_{\textup{sos}}$ construction above to
combinatorial optimization problems. We will explain how to do this through
an example.

The {\em stable set problem} is, given a graph $G = (V,E)$, to find the
maximum size of a vertex set which is completely disconnected. We can embed
this problem in $\RR^n$ as follows. Let $n = |V|$ and label the vertices by 
$1, \ldots, n$. For each stable set $S \subseteq V$, let $\chi_S$ be its 
characteristic vector: $(\chi_S)_i = 1$ if $i \in S$, and 0 if $i \notin S$.
The collection $X$ of these characteristic vectors forms a discrete point set
in $\RR^n$, and in fact it is an algebraic variety. Its ideal
$I$ is generated by $\{x_i^2 - x_i \, (1\le i \le n);\, x_ix_j\, (ij \in E) \}$.

The problem of finding the maximum size of a stable set can now be rephrased
over $\RR^n$ as finding the minimum value of the function
$f(x) = - \sum_{i=1}^n x_i$ over the set $X$ given above. To use the
$f_{\textup{sos}}$ idea, we need to define a notion of being a sum of squares
on $X$. As long as $X$ is an algebraic set, this is accomplished in 
\cite{glpt}. The method is to take a basis $B$ for $\RR[x]/I$ and use 
semidefinite programming to calculate the space of sums of squares mod $I$ by
computing in the basis $B$. One caveat is that we need to restrict the 
squares to some bounded degree $k$ to get a polynomial-time algorithm. If
we set $k=1$ we get the celebrated Lovasz theta body for the stable set
problem, which was the inspiration for \cite{glpt}. By increasing $k$ we
increase the runtime of our algorithm but potentially improve its accuracy;
this is discussed in chapter X. 

\subsection{Structure of the Thesis}
In the remainder of the introduction, we give an extended abstract of each
chapter. We will describe the problem and its motivation and applications,
and give a precise statement of results and an indication of how they were 
proved.

Of course, the papers themselves are present as the other chapters here, so
they are available to also read. However, the goal of this introduction
is to give a precise overview of results and be a guide to the rest of the
thesis.

\section{A Semidefinite Approach to the $K_\MakeLowercase{i}$ Cover Problem}

Extended abstract of the Ki cover paper goes here.

\section{A Note on Notation}
The following chapters were published as separate papers, and in some cases
refer to different aspects of the same or related problems. As such, the 
chapters use different notation to fit the topic at hand, so the same object
may have different names in different chapters.
